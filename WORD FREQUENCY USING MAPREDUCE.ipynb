{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# <center>IMPORTING NECESSARY PACKAGES</center>"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import re\nimport string\nimport numpy as np\nimport pandas as pd",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# <br><br><center>DATA CLEANING</center>"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def Data_Cleaning(text_list):\n    \n    output = []\n    \n    # Extracting individual lines from the list\n    for line in text_list:\n        \n        # Ignoring empty lines\n        if line!=\"\\n\":\n            \n            # Using regex to replace non characters & single space with space.\n            # Replacing with space to avoid concatenating words which have just punctuations between them\n            # Eg: \"myself--for\" in line 821 \n            cleaned_line = re.sub(r'[^a-zA-Z ]',\" \",line).lower()\n            \n    \n            # If line ends with a number (\"Chapter 1\") the word is followed by a space in the end after cleaning (\"chapter \")\n            # Removing such space by checking if the last character of the line is just a space \n            if cleaned_line.isspace()!= True:\n                if cleaned_line[-1:]==\" \":\n                    output.append(cleaned_line[:-1])\n                else:\n                    output.append(cleaned_line)\n\n    return output",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# <br><br><center>DATA SPLIT</center>"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def Data_Split(cleansed_data):\n    \n    # Slicing first 5000 lines\n    output1 = cleansed_data[0:5000]\n    \n    # Slicing remaining lines\n    output2 = cleansed_data[5000:]\n    return output1, output2",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# <br><br><center>MAPPER</center>"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def Mapper1(text_lines):\n    \n    output = []\n    \n    # Iterating each individual line from the list\n    for lines in text_lines:\n        \n        # Iterating each word in the line\n        for word in lines.split(\" \"):\n            # Ignoring if the word is an empty character\n            if word != \"\":\n                #  Creating a dictionay with word as key and value as 1 and appending the dictionary to the list\n                output.append({word: 1})\n    \n    return output\n\n\n\n# Same logic as Mapper1 function above\ndef Mapper2(text_lines):\n    output = []\n    \n    for lines in text_lines:\n        for word in lines.split(\" \"):\n            if word != \"\":\n                output.append({word: 1})\n\n        \n    return output",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# <br><br><center>SORT MAPPER OUTPUTS</center>"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def Sort_Mapper(list_1, list_2):\n    \n    # Combining both lists\n    combined_list = list_1 + list_2\n    \n    words = []\n    \n    # Iterating each dictionary in the list\n    for dictionaries in combined_list:\n        \n        # Selecting keys(words) of individual dictionary and appending them to a list\n        for k, v in dictionaries.items():\n            words.append(k)\n    \n    # Extracting the indices by sorting the words alphabeltically\n    sorted_indices = list(np.argsort(words))\n    \n    output = []\n    \n    # Using the sorted indices to rearrange and store the dictionaries of the combined list in a new list\n    for i in range(len(combined_list)):\n        output.append(combined_list[sorted_indices[i]])\n    \n    return output",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# <br><br><center>PARTITION</center>"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def Partition(sorted_list):\n    \n    # Checking the index for the first key(word) which starts with \"n\"\n    for i in range(len(sorted_list)):\n        if str(sorted_list[i].keys())[12] == \"n\":\n            n_index = i\n            break\n    \n    # List 1 contains keys(words) starting with letters \"a\" to \"m\"\n    # List 2 contains keys(words) starting with letters \"n\" to \"z\"\n    output1, output2 = sorted_list[:n_index], sorted_list[n_index:]\n    \n    return output1, output2",
            "execution_count": 6,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# <br><br><center>REDUCER</center>"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def Reducer1(partition_output):\n    words_list = []\n    \n    # Appending keys from all the dictionaries in a ist  \n    for dictionaries in partition_output:\n        for k,v in dictionaries.items():\n            words_list.append(k)\n    \n    # Creating a list with a set of individual words \n    individual_words = sorted(list(set(words_list)))\n    index = 0\n    \n    # Dictionary to store key and its concatenated values\n    reducer_dict = {}\n    \n    # Concatenating key with all its values similar to the reducer function. Eg: {\"a\":[1,1,1,1....] , \"b\":[1,1,1,1....] , .....} \n    for word in individual_words:\n        values = []\n    \n        for text_word in words_list[index:]:\n        \n            if text_word == word:\n                values.append(int(str(partition_output[index].values())[-3]))\n                index+=1\n        \n            if text_word != word or index == len(words_list):\n                reducer_dict[word]=values\n                break\n    \n    ct=[]\n    \n    # Counting the frequency for each word by checking length of list with the concatenated values\n    for word in individual_words:\n        ct.append(len(reducer_dict[word]))    \n    \n    return individual_words, ct\n\n\n# Implemeted using same logis as Reducer1 function\ndef Reducer2(partition_output):\n    words_list = []\n\n    for dictionaries in partition_output:\n        for k,v in dictionaries.items():\n            words_list.append(k)\n\n    individual_words = sorted(list(set(words_list)))\n    index = 0\n    reducer_dict = {}\n\n    for word in individual_words:\n        values = []\n    \n        for text_word in words_list[index:]:\n        \n            if text_word == word:\n                values.append(int(str(partition_output[index].values())[-3]))\n                index+=1\n        \n            if text_word != word or index == len(words_list):\n                reducer_dict[word]=values\n                break\n    \n    ct=[]\n    \n    for word in individual_words:\n        ct.append(len(reducer_dict[word]))    \n    \n    return individual_words, ct",
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# <br><br><center>MAIN FUNCTION (<font color =\"blue\">MAPREDUCE</font>)</center>"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def Map_Reduce(file_name):\n    \n    # Storing each line of the text as an individual elemnt of  a list \n    with open(file_name,\"r\") as file:\n        book = file.readlines()\n    \n    # Calling Data Cleaning function\n    cleansed_data = Data_Cleaning(book)\n    \n    # Calling Data Split  to split data on 5000th line\n    part1 , part2 = Data_Split(cleansed_data)\n    \n    # Calling both mapper functions with the split data as arguments to form key value pairs\n    mapper1_output , mapper2_output = Mapper1(part1), Mapper2(part2)\n    \n    # Calling Sort function to sort the combined output of both mappers \n    sort_output = Sort_Mapper(mapper1_output , mapper2_output)\n    \n    # Calling partition function to split sorted result into two lists, \n    # one for words starting with \"a\" to \"m\" and another for words starting with \"n\" to \"z\"\n    partition_output1 , partition_output2 = Partition(sort_output)\n    \n    # Calling reducer function to count the frequency of each individual word\n    reducer1_words , reducer1_frequency = Reducer1(partition_output1)\n    reducer2_words , reducer2_frequency = Reducer2(partition_output2)\n\n    # Concatenating the reducer output into a dataframe\n    final_output = pd.DataFrame()\n    final_output[\"Word\"] = reducer1_words + reducer2_words\n    final_output[\"Frequency\"] = reducer1_frequency + reducer2_frequency\n    \n    # Storing the results in a csv file\n    final_output.to_csv(\"Pride and Prejudice.csv\", index=False)\n    \n    return final_output",
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# <br><br><center>CALLING MAIN (<font color =\"blue\">MAPREDUCE</font>) FUNCTION</center>"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Enter name of file. File needs to be stored in the current working directory\nfile_name = \"Pride_and_Prejudice.txt\"\n\n# Calling the Main Function (Map_Reduce)\nMap_Reduce_Output = Map_Reduce(file_name)",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Map_Reduce_Output.head(11)",
            "execution_count": 10,
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a</td>\n      <td>1954</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>abatement</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>abhorrence</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>abhorrent</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>abide</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>abiding</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>abilities</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>able</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ablution</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>abode</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>abominable</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "          Word  Frequency\n0            a       1954\n1    abatement          1\n2   abhorrence          6\n3    abhorrent          1\n4        abide          1\n5      abiding          1\n6    abilities          6\n7         able         54\n8     ablution          1\n9        abode          8\n10  abominable          6"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Top 10 Least frequent words\nMap_Reduce_Output.sort_values([\"Frequency\"], ascending=True, axis=0).head(10)",
            "execution_count": 11,
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3129</th>\n      <td>involves</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3204</th>\n      <td>kindled</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3207</th>\n      <td>kindred</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3208</th>\n      <td>kinds</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3211</th>\n      <td>kiss</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3213</th>\n      <td>kitchen</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3215</th>\n      <td>knees</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3223</th>\n      <td>kympton</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3225</th>\n      <td>labour</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3226</th>\n      <td>lace</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "          Word  Frequency\n3129  involves          1\n3204   kindled          1\n3207   kindred          1\n3208     kinds          1\n3211      kiss          1\n3213   kitchen          1\n3215     knees          1\n3223   kympton          1\n3225    labour          1\n3226      lace          1"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Top 10 Most frequent words\nMap_Reduce_Output.sort_values([\"Frequency\"], ascending=False, axis=0).head(10)",
            "execution_count": 12,
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5590</th>\n      <td>the</td>\n      <td>4331</td>\n    </tr>\n    <tr>\n      <th>5656</th>\n      <td>to</td>\n      <td>4163</td>\n    </tr>\n    <tr>\n      <th>3819</th>\n      <td>of</td>\n      <td>3611</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>and</td>\n      <td>3585</td>\n    </tr>\n    <tr>\n      <th>2709</th>\n      <td>her</td>\n      <td>2225</td>\n    </tr>\n    <tr>\n      <th>2806</th>\n      <td>i</td>\n      <td>2070</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>a</td>\n      <td>1954</td>\n    </tr>\n    <tr>\n      <th>2890</th>\n      <td>in</td>\n      <td>1880</td>\n    </tr>\n    <tr>\n      <th>6064</th>\n      <td>was</td>\n      <td>1846</td>\n    </tr>\n    <tr>\n      <th>5045</th>\n      <td>she</td>\n      <td>1710</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "     Word  Frequency\n5590  the       4331\n5656   to       4163\n3819   of       3611\n268   and       3585\n2709  her       2225\n2806    i       2070\n0       a       1954\n2890   in       1880\n6064  was       1846\n5045  she       1710"
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<font size=3>Kindly find the book <b>Pride and Prejudice</b> along with the dataframe of words and their corresponding frequency exported to a csv file named <b>Pride and Prejudice.csv</b> in my GitHub repository <a href=\"https://github.com/Prasanna-Vengatesh-Venkataraman/WORD-FREQUENCY-USING-MAPREDUCE\">here</a>.</font>"
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}